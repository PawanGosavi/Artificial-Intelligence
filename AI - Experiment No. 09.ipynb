{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AI P9  - Pawan Gosavi","provenance":[{"file_id":"1zoKa6lGNCoUxSBu_vUYtbVmhT0r6s7mD","timestamp":1630917295555},{"file_id":"1cEKKtns_iqBdoHzrAJvAEXyRpJYl1z8P","timestamp":1630661705516},{"file_id":"1QA-OqMuy52mVrkpcUnEUBmZp6VbRSAwG","timestamp":1630604280882},{"file_id":"1O5SRpLxD_T2ZSV7wY9b8mb_Eit3ONA4t","timestamp":1629725933622},{"file_id":"1pFLQmx1OugTd0C4uChBG2TpVlUZqUfVw","timestamp":1629656109524},{"file_id":"1Xe_fFe2qoVdIVWFsKHaYNtOqMC5BH8dI","timestamp":1628426337969},{"file_id":"17HJWngBVSOL1-hnlFCSiLwS2zUBYGqxc","timestamp":1627893770523},{"file_id":"1xRRB9oanS_2FDx_-A0BKIt4QLNeT6D-b","timestamp":1627397053206}],"collapsed_sections":[],"authorship_tag":"ABX9TyOXqQVcF/avQoVNDIT5MiOv"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KnkBmkZSV4v2"},"source":["# **Practical No.09**"]},{"cell_type":"markdown","metadata":{"id":"P8qOBgOzRckd"},"source":["## **Roll No. 01**\n","## **Name : Pawan Gosavi**"]},{"cell_type":"markdown","metadata":{"id":"VG90RjAOWC9v"},"source":["## **Aim :\tImplement Passive Reinforcement Learning Algorithm based on Adaptive Dynamic Programming (ADP) for the 3 x 4 World Problem.**"]},{"cell_type":"code","metadata":{"id":"9fWS5MsKQQbO","executionInfo":{"status":"ok","timestamp":1630922547291,"user_tz":-330,"elapsed":8,"user":{"displayName":"Pawan Gosavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8RyeyM4StH-pctbqDyYUhzU-WWiSP6TP1x0Qj=s64","userId":"05422985579146096715"}}},"source":["import sys\n","import operator\n","from collections import defaultdict\n","import random"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"_2FTDoA0Qm-c","executionInfo":{"status":"ok","timestamp":1630922548098,"user_tz":-330,"elapsed":814,"user":{"displayName":"Pawan Gosavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8RyeyM4StH-pctbqDyYUhzU-WWiSP6TP1x0Qj=s64","userId":"05422985579146096715"}}},"source":["class MDP:\n","    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n","    and reward function. We also keep track of a gamma value, for use by\n","    algorithms. The transition model is represented somewhat differently from\n","    the text. Instead of P(s' | s, a) being a probability number for each\n","    state/state/action triplet, we instead have T(s, a) return a\n","    list of (p, s') pairs. We also keep track of the possible states,\n","    terminal states, and actions for each state. [Page 646]\"\"\"\n","\n","    def __init__(self, init, actlist, terminals, transitions=None, reward=None, states=None, gamma=0.9):\n","        if not (0 < gamma <= 1):\n","            raise ValueError(\"An MDP must have 0 < gamma <= 1\")\n","\n","        # collect states from transitions table if not passed.\n","        self.states = states or self.get_states_from_transitions(transitions)\n","\n","        self.init = init\n","\n","        if isinstance(actlist, list):\n","            # if actlist is a list, all states have the same actions\n","            self.actlist = actlist\n","\n","        elif isinstance(actlist, dict):\n","            # if actlist is a dict, different actions for each state\n","            self.actlist = actlist\n","\n","        self.terminals = terminals\n","        self.transitions = transitions or {}\n","        if not self.transitions:\n","            print(\"Warning: Transition table is empty.\")\n","\n","        self.gamma = gamma\n","\n","        self.reward = reward or {s: 0 for s in self.states}\n","\n","        # self.check_consistency()\n","\n","    def R(self, state):\n","        \"\"\"Return a numeric reward for this state.\"\"\"\n","\n","        return self.reward[state]\n","\n","    def T(self, state, action):\n","        \"\"\"Transition model. From a state and an action, return a list\n","        of (probability, result-state) pairs.\"\"\"\n","\n","        if not self.transitions:\n","            raise ValueError(\"Transition model is missing\")\n","        else:\n","            return self.transitions[state][action]\n","\n","    def actions(self, state):\n","        \"\"\"Return a list of actions that can be performed in this state. By default, a\n","        fixed list of actions, except for terminal states. Override this\n","        method if you need to specialize by state.\"\"\"\n","\n","        if state in self.terminals:\n","            return [None]\n","        else:\n","            return self.actlist\n","\n","    def get_states_from_transitions(self, transitions):\n","        if isinstance(transitions, dict):\n","            s1 = set(transitions.keys())\n","            s2 = set(tr[1] for actions in transitions.values()\n","                     for effects in actions.values()\n","                     for tr in effects)\n","            return s1.union(s2)\n","        else:\n","            print('Could not retrieve states from transitions')\n","            return None\n","\n","    def check_consistency(self):\n","\n","        # check that all states in transitions are valid\n","        assert set(self.states) == self.get_states_from_transitions(self.transitions)\n","\n","        # check that init is a valid state\n","        assert self.init in self.states\n","\n","        # check reward for each state\n","        assert set(self.reward.keys()) == set(self.states)\n","\n","        # check that all terminals are valid states\n","        assert all(t in self.states for t in self.terminals)\n","\n","        # check that probability distributions for all actions sum to 1\n","        for s1, actions in self.transitions.items():\n","            for a in actions.keys():\n","                s = 0\n","                for o in actions[a]:\n","                    s += o[0]\n","                assert abs(s - 1) < 0.001\n","\n","class MDP2(MDP):\n","    \"\"\"\n","    Inherits from MDP. Handles terminal states, and transitions to and from terminal states better.\n","    \"\"\"\n","\n","    def __init__(self, init, actlist, terminals, transitions, reward=None, gamma=0.9):\n","        MDP.__init__(self, init, actlist, terminals, transitions, reward, gamma=gamma)\n","\n","    def T(self, state, action):\n","        if action is None:\n","            return [(0.0, state)]\n","        else:\n","            return self.transitions[state][action]\n","\n","class GridMDP(MDP):\n","    \"\"\"A two-dimensional grid MDP, as in [Figure 17.1]. All you have to do is\n","    specify the grid as a list of lists of rewards; use None for an obstacle\n","    (unreachable state). Also, you should specify the terminal states.\n","    An action is an (x, y) unit vector; e.g. (1, 0) means move east.\"\"\"\n","\n","    def __init__(self, grid, terminals, init=(0, 0), gamma=.9):\n","        grid.reverse()  # because we want row 0 on bottom, not on top\n","        reward = {}\n","        states = set()\n","        self.rows = len(grid)\n","        self.cols = len(grid[0])\n","        self.grid = grid\n","        for x in range(self.cols):\n","            for y in range(self.rows):\n","                if grid[y][x]:\n","                    states.add((x, y))\n","                    reward[(x, y)] = grid[y][x]\n","        self.states = states\n","        actlist = orientations\n","        transitions = {}\n","        for s in states:\n","            transitions[s] = {}\n","            for a in actlist:\n","                transitions[s][a] = self.calculate_T(s, a)\n","        MDP.__init__(self, init, actlist=actlist,\n","                     terminals=terminals, transitions=transitions,\n","                     reward=reward, states=states, gamma=gamma)\n","\n","    def calculate_T(self, state, action):\n","        if action:\n","            return [(0.8, self.go(state, action)),\n","                    (0.1, self.go(state, turn_right(action))),\n","                    (0.1, self.go(state, turn_left(action)))]\n","        else:\n","            return [(0.0, state)]\n","\n","    def T(self, state, action):\n","        return self.transitions[state][action] if action else [(0.0, state)]\n","\n","    def go(self, state, direction):\n","        \"\"\"Return the state that results from going in this direction.\"\"\"\n","\n","        state1 = vector_add(state, direction)\n","        return state1 if state1 in self.states else state\n","\n","    def to_grid(self, mapping):\n","        \"\"\"Convert a mapping from (x, y) to v into a [[..., v, ...]] grid.\"\"\"\n","\n","        return list(reversed([[mapping.get((x, y), None)\n","                               for x in range(self.cols)]\n","                              for y in range(self.rows)]))\n","\n","    def to_arrows(self, policy):\n","        chars = {(1, 0): '>', (0, 1): '^', (-1, 0): '<', (0, -1): 'v', None: '.'}\n","        return self.to_grid({s: chars[a] for (s, a) in policy.items()})"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"j2keZuP0RoQL","executionInfo":{"status":"ok","timestamp":1630922548099,"user_tz":-330,"elapsed":32,"user":{"displayName":"Pawan Gosavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8RyeyM4StH-pctbqDyYUhzU-WWiSP6TP1x0Qj=s64","userId":"05422985579146096715"}}},"source":["# Grid Functions\n","\n","\n","orientations = EAST, NORTH, WEST, SOUTH = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n","turns = LEFT, RIGHT = (+1, -1)\n","\n","\n","def turn_heading(heading, inc, headings=orientations):\n","    return headings[(headings.index(heading) + inc) % len(headings)]\n","\n","\n","def turn_right(heading):\n","    return turn_heading(heading, RIGHT)\n","\n","\n","def turn_left(heading):\n","    return turn_heading(heading, LEFT)\n","\n","\n","def distance(a, b):\n","    \"\"\"The distance between two (x, y) points.\"\"\"\n","    xA, yA = a\n","    xB, yB = b\n","    return np.hypot((xA - xB), (yA - yB))\n","\n","\n","def distance_squared(a, b):\n","    \"\"\"The square of the distance between two (x, y) points.\"\"\"\n","    xA, yA = a\n","    xB, yB = b\n","    return (xA - xB) ** 2 + (yA - yB) ** 2"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"HUbzfhjZTCDB","executionInfo":{"status":"ok","timestamp":1630922548099,"user_tz":-330,"elapsed":31,"user":{"displayName":"Pawan Gosavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8RyeyM4StH-pctbqDyYUhzU-WWiSP6TP1x0Qj=s64","userId":"05422985579146096715"}}},"source":["# Other Funtions\n","\n","def policy_evaluation(pi, U, mdp, k=20):\n","    \"\"\"Return an updated utility mapping U from each state in the MDP to its\n","    utility, using an approximation (modified policy iteration).\"\"\"\n","\n","    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n","    for i in range(k):\n","        for s in mdp.states:\n","            U[s] = R(s) + gamma * sum(p * U[s1] for (p, s1) in T(s, pi[s]))\n","    return U\n","\n","def vector_add(a, b):\n","    \"\"\"Component-wise addition of two vectors.\"\"\"\n","    return tuple(map(operator.add, a, b))"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Stl-A8JsQKqz","executionInfo":{"status":"ok","timestamp":1630922548100,"user_tz":-330,"elapsed":32,"user":{"displayName":"Pawan Gosavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8RyeyM4StH-pctbqDyYUhzU-WWiSP6TP1x0Qj=s64","userId":"05422985579146096715"}}},"source":["# Adaptive Dynamic Programming Agent\n","\n","class PassiveADPAgent:\n","    \"\"\"\n","    [Figure 21.2]\n","    Passive (non-learning) agent that uses adaptive dynamic programming\n","    on a given MDP and policy.\n","    import sys\n","    from mdp import sequential_decision_environment\n","    north = (0, 1)\n","    south = (0,-1)\n","    west = (-1, 0)\n","    east = (1, 0)\n","    policy = {(0, 2): east, (1, 2): east, (2, 2): east, (3, 2): None, (0, 1): north, (2, 1): north,\n","              (3, 1): None, (0, 0): north, (1, 0): west, (2, 0): west, (3, 0): west,}\n","    agent = PassiveADPAgent(policy, sequential_decision_environment)\n","    for i in range(100):\n","        run_single_trial(agent,sequential_decision_environment)\n","    agent.U[(0, 0)] > 0.2\n","    True\n","    agent.U[(0, 1)] > 0.2\n","    True\n","    \"\"\"\n","\n","    class ModelMDP(MDP):\n","        \"\"\"Class for implementing modified Version of input MDP with\n","        an editable transition model P and a custom function T.\"\"\"\n","\n","        def __init__(self, init, actlist, terminals, gamma, states):\n","            super().__init__(init, actlist, terminals, states=states, gamma=gamma)\n","            nested_dict = lambda: defaultdict(nested_dict)\n","            # StackOverflow:whats-the-best-way-to-initialize-a-dict-of-dicts-in-python\n","            self.P = nested_dict()\n","\n","        def T(self, s, a):\n","            \"\"\"Return a list of tuples with probabilities for states\n","            based on the learnt model P.\"\"\"\n","            return [(prob, res) for (res, prob) in self.P[(s, a)].items()]\n","\n","    def __init__(self, pi, mdp):\n","        self.pi = pi\n","        self.mdp = PassiveADPAgent.ModelMDP(mdp.init, mdp.actlist,\n","                                            mdp.terminals, mdp.gamma, mdp.states)\n","        self.U = {}\n","        self.Nsa = defaultdict(int)\n","        self.Ns1_sa = defaultdict(int)\n","        self.s = None\n","        self.a = None\n","        self.visited = set()  # keeping track of visited states\n","\n","    def __call__(self, percept):\n","        s1, r1 = percept\n","        mdp = self.mdp\n","        R, P, terminals, pi = mdp.reward, mdp.P, mdp.terminals, self.pi\n","        s, a, Nsa, Ns1_sa, U = self.s, self.a, self.Nsa, self.Ns1_sa, self.U\n","\n","        if s1 not in self.visited:  # Reward is only known for visited state.\n","            U[s1] = R[s1] = r1\n","            self.visited.add(s1)\n","        if s is not None:\n","            Nsa[(s, a)] += 1\n","            Ns1_sa[(s1, s, a)] += 1\n","            # for each t such that Ns′|sa [t, s, a] is nonzero\n","            for t in [res for (res, state, act), freq in Ns1_sa.items()\n","                      if (state, act) == (s, a) and freq != 0]:\n","                P[(s, a)][t] = Ns1_sa[(t, s, a)] / Nsa[(s, a)]\n","\n","        self.U = policy_evaluation(pi, U, mdp)\n","        ##\n","        ##\n","        self.Nsa, self.Ns1_sa = Nsa, Ns1_sa\n","        if s1 in terminals:\n","            self.s = self.a = None\n","        else:\n","            self.s, self.a = s1, self.pi[s1]\n","        return self.a\n","\n","    def update_state(self, percept):\n","        \"\"\"To be overridden in most cases. The default case\n","        assumes the percept to be of type (state, reward).\"\"\"\n","        return percept"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"t6MqQfhDSuQV","executionInfo":{"status":"ok","timestamp":1630922548101,"user_tz":-330,"elapsed":32,"user":{"displayName":"Pawan Gosavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8RyeyM4StH-pctbqDyYUhzU-WWiSP6TP1x0Qj=s64","userId":"05422985579146096715"}}},"source":["# Execution Function\n","\n","def run_single_trial(agent_program, mdp):\n","    \"\"\"Execute trial for given agent_program\n","    and mdp. mdp should be an instance of subclass\n","    of mdp.MDP \"\"\"\n","\n","    def take_single_action(mdp, s, a):\n","        \"\"\"\n","        Select outcome of taking action a\n","        in state s. Weighted Sampling.\n","        \"\"\"\n","        x = random.uniform(0, 1)\n","        cumulative_probability = 0.0\n","        for probability_state in mdp.T(s, a):\n","            probability, state = probability_state\n","            cumulative_probability += probability\n","            if x < cumulative_probability:\n","                break\n","        return state\n","\n","    current_state = mdp.init\n","    while True:\n","        current_reward = mdp.R(current_state)\n","        percept = (current_state, current_reward)\n","        next_action = agent_program(percept)\n","        if next_action is None:\n","            break\n","        current_state = take_single_action(mdp, current_state, next_action)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"jNEpmadlQxkb","executionInfo":{"status":"ok","timestamp":1630922548101,"user_tz":-330,"elapsed":32,"user":{"displayName":"Pawan Gosavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8RyeyM4StH-pctbqDyYUhzU-WWiSP6TP1x0Qj=s64","userId":"05422985579146096715"}}},"source":["# A 4x3 grid environment that presents the agent with a sequential decision problem.\n","\n","sequential_decision_environment = GridMDP([[-0.04, -0.04, -0.04, +1],\n","                                           [-0.04, None, -0.04, -1],\n","                                           [-0.04, -0.04, -0.04, -0.04]],\n","                                          terminals=[(3, 2), (3, 1)])"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_BBbj2mLUmos","executionInfo":{"status":"ok","timestamp":1630922548102,"user_tz":-330,"elapsed":33,"user":{"displayName":"Pawan Gosavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8RyeyM4StH-pctbqDyYUhzU-WWiSP6TP1x0Qj=s64","userId":"05422985579146096715"}},"outputId":"ed13dbb2-134d-4330-f9fe-afb7863129cc"},"source":["print(\"\\n3 x 4 World Problem :\\n\")\n","print(sequential_decision_environment.reward)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","3 x 4 World Problem :\n","\n","{(0, 0): -0.04, (0, 1): -0.04, (0, 2): -0.04, (1, 0): -0.04, (1, 2): -0.04, (2, 0): -0.04, (2, 1): -0.04, (2, 2): -0.04, (3, 0): -0.04, (3, 1): -1, (3, 2): 1}\n"]}]},{"cell_type":"code","metadata":{"id":"7V-Hn0cmSKOC","executionInfo":{"status":"ok","timestamp":1630922548103,"user_tz":-330,"elapsed":31,"user":{"displayName":"Pawan Gosavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8RyeyM4StH-pctbqDyYUhzU-WWiSP6TP1x0Qj=s64","userId":"05422985579146096715"}}},"source":["# Action Directions\n","\n","north = (0, 1)\n","south = (0,-1)\n","west = (-1, 0)\n","east = (1, 0)\n","\n","policy = {\n","    (0, 2): east,  (1, 2): east,  (2, 2): east,   (3, 2): None,\n","    (0, 1): north,                (2, 1): north,  (3, 1): None,\n","    (0, 0): north, (1, 0): west,  (2, 0): west,   (3, 0): west, \n","}"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uyWEjUZVSUM9","executionInfo":{"status":"ok","timestamp":1630922548104,"user_tz":-330,"elapsed":31,"user":{"displayName":"Pawan Gosavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8RyeyM4StH-pctbqDyYUhzU-WWiSP6TP1x0Qj=s64","userId":"05422985579146096715"}},"outputId":"b6872be2-f938-4821-be59-7cbb2b6fb652"},"source":["ADPagent = PassiveADPAgent(policy, sequential_decision_environment)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: Transition table is empty.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WYmnLDLdjCQv","executionInfo":{"status":"ok","timestamp":1630922559703,"user_tz":-330,"elapsed":11626,"user":{"displayName":"Pawan Gosavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8RyeyM4StH-pctbqDyYUhzU-WWiSP6TP1x0Qj=s64","userId":"05422985579146096715"}},"outputId":"99f0f043-69dd-4731-8a51-afd59378d433"},"source":["trialsnum = int(input(\"How Many trials To You Want e.g 200 ? : \"))\n","\n","for i in range(trialsnum):\n","    run_single_trial(ADPagent, sequential_decision_environment)"],"execution_count":11,"outputs":[{"name":"stdout","output_type":"stream","text":["How Many trials To You Want e.g 200 ? : 200\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PgIK_wZsTaU-","executionInfo":{"status":"ok","timestamp":1630922559704,"user_tz":-330,"elapsed":14,"user":{"displayName":"Pawan Gosavi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8RyeyM4StH-pctbqDyYUhzU-WWiSP6TP1x0Qj=s64","userId":"05422985579146096715"}},"outputId":"d091240c-40ed-4b6e-ee0b-30b253ab5ff5"},"source":["# Solution\n","print(\"\\nSolution using Adaptive Dynamic Programming : \\n\")\n","print('\\n'.join([str(k)+':'+str(v) for k, v in ADPagent.U.items()]))"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Solution using Adaptive Dynamic Programming : \n","\n","(0, 0):0.30079165602998575\n","(0, 1):0.4049633027862025\n","(1, 2):0.6511295886302265\n","(3, 2):1.0\n","(3, 0):0.0\n","(3, 1):-1.0\n","(2, 1):0.4855176390447414\n","(2, 0):0.0\n","(2, 2):0.808215066042189\n","(1, 0):0.2178408069486689\n","(0, 2):0.5162578770779896\n"]}]}]}